{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh2QFfiGO105"
      },
      "source": [
        "**Preprocessing and baseline model**"
      ],
      "id": "kh2QFfiGO105"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*https://www.kaggle.com/competitions/hms-harmful-brain-activity-classification/overview*"
      ],
      "metadata": {
        "id": "I9a0u_6p4VjC"
      },
      "id": "I9a0u_6p4VjC"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "67116254"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
        "from scipy.stats import entropy\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import random"
      ],
      "id": "67116254"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHMjHuDwPer5",
        "outputId": "9087e6e9-a8b7-47ff-ab45-f98f191309f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "gHMjHuDwPer5"
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/My Drive/HMS/train_spectrograms/'"
      ],
      "metadata": {
        "id": "8xE98aGetmTY"
      },
      "id": "8xE98aGetmTY",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cfC9yASAPejd"
      },
      "outputs": [],
      "source": [
        "train_file_path = '/content/drive/My Drive/HMS/train.csv'"
      ],
      "id": "cfC9yASAPejd"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_2Amn8x6PefD"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(train_file_path)"
      ],
      "id": "_2Amn8x6PefD"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3tx-GwoPu7f",
        "outputId": "b5350897-3ee5-4875-d7c1-2df53a5a280c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of unique values:11138\n"
          ]
        }
      ],
      "source": [
        "list_unique_values = list(df['spectrogram_id'].unique())\n",
        "print(f\"number of unique values:{len(list_unique_values)}\")"
      ],
      "id": "j3tx-GwoPu7f"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVszC9ocXUAl",
        "outputId": "4c14dec4-c040-4843-aa5f-8114a5596ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of random unique values:100\n"
          ]
        }
      ],
      "source": [
        "# for ranom generation, if smaller array are rquired\n",
        "random.seed(42)\n",
        "list_unique_value_random = random.sample(list_unique_values, 100)\n",
        "print(f\"number of random unique values:{len(list_unique_value_random)}\")"
      ],
      "id": "cVszC9ocXUAl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rInBCy2oP0X4"
      },
      "source": [
        "load spectograms parquets according to unique_values with names 4653464<br>"
      ],
      "id": "rInBCy2oP0X4"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTD55lsI7zwA",
        "outputId": "21465baf-71bf-4f2c-af98-198caa7e727f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading Parquet Files: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed in 138.89 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def load_parquet_files(unique_values, base_path):\n",
        "    \"\"\"\n",
        "    Loads parquet files from a specified base path into a dictionary.\n",
        "\n",
        "    This function iterates over a list of unique values, constructs a file path for each corresponding\n",
        "    parquet file in the specified base directory, and attempts to load the file into a pandas DataFrame.\n",
        "    Each successfully loaded DataFrame is stored in a dictionary with its unique value as the key.\n",
        "    The function tracks the time taken to load all files and prints the duration upon completion.\n",
        "    If a file cannot be loaded, an error message is printed.\n",
        "\n",
        "    Parameters:\n",
        "    - unique_values (list): A list of unique values used to identify the parquet files to be loaded.\n",
        "                            Each value corresponds to a part of the filename for a parquet file.\n",
        "    - base_path (str): The base directory path where the parquet files are stored. Each parquet file\n",
        "                       is expected to be named using its unique value from the unique_values list and\n",
        "                       have a '.parquet' extension.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary where each key is a unique value from the unique_values list and each value\n",
        "            is a pandas DataFrame loaded from the corresponding parquet file.\n",
        "\n",
        "    Example usage:\n",
        "    # Define the list of unique values and base path\n",
        "    unique_values = [924234, 1219001, 353733]\n",
        "    base_path = '/content/drive/My Drive/HMS/train_spectrograms/'\n",
        "\n",
        "    # Load the parquet files into a dictionary\n",
        "    parquet_dict = load_parquet_files(unique_values, base_path)\n",
        "\n",
        "    \"\"\"\n",
        "    parquet_dict = {}\n",
        "    start_time = time.time()\n",
        "    for value in tqdm(unique_values, desc=\"Loading Parquet Files\"):\n",
        "        file_path = f\"{base_path}/{value}.parquet\"\n",
        "        try:\n",
        "            parquet_data = pd.read_parquet(file_path)\n",
        "            parquet_dict[value] = parquet_data\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_path}: {e}\")\n",
        "    end_time = time.time()\n",
        "    print(f\"Completed in {end_time - start_time:.2f} seconds\")\n",
        "    return parquet_dict\n",
        "# parquet_dict = load_parquet_files(list_unique_values, base_path )\n",
        "parquet_dict = load_parquet_files(list_unique_value_random, base_path ) # smaller  random\n"
      ],
      "id": "aTD55lsI7zwA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qT-msKjQUJm"
      },
      "source": [
        "expanded parquest with subset names as 87687_2<br>\n",
        "and values of the 50 lines by subset <br>\n"
      ],
      "id": "0qT-msKjQUJm"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdWD2ovhAYSu",
        "outputId": "b8c52412-1670-46be-b203-5655ff3db3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Expanding Parquet Dict: 100%|██████████| 106800/106800 [00:12<00:00, 8609.30it/s] \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def expand_parquet_dict(parquet_dict, df):\n",
        "    \"\"\"\n",
        "    Expands the data in a dictionary of parquet files by selecting specific data\n",
        "    based on spectrogram identifiers and label offset information from a DataFrame.\n",
        "\n",
        "    This function iterates over each row in a DataFrame that contains spectrogram IDs,\n",
        "    sub-IDs, and label offset seconds. It uses these to construct a unique key for each\n",
        "    piece of data and selects a specific range of data from the corresponding parquet file\n",
        "    in the parquet_dict. The selected data is then stored in a new dictionary with the unique\n",
        "    key as its identifier.\n",
        "\n",
        "    Parameters:\n",
        "    - parquet_dict (dict): A dictionary where keys are spectrogram IDs and values are DataFrames\n",
        "                           loaded from parquet files.\n",
        "    - df (pd.DataFrame): A DataFrame containing at least the columns 'spectrogram_id',\n",
        "                         'spectrogram_sub_id', and 'spectrogram_label_offset_seconds'. These columns\n",
        "                         are used to identify which data to select from each parquet file.\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary where each key is a unique identifier composed of the spectrogram ID and\n",
        "            sub-ID, and each value is a selected portion of the data from the corresponding parquet\n",
        "            file, based on the label offset seconds.\n",
        "\n",
        "    Example usage:\n",
        "    expanded_parquet_dict = expand_parquet_dict(parquet_dict, df)\n",
        "    \"\"\"\n",
        "    expanded_parquet_dict = {}\n",
        "    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Expanding Parquet Dict\"):\n",
        "        spectrogram_id = row['spectrogram_id']\n",
        "        spectrogram_sub_id = row['spectrogram_sub_id']\n",
        "        label_offset_seconds = row['spectrogram_label_offset_seconds']\n",
        "\n",
        "        unique_key = f\"{spectrogram_id}_{spectrogram_sub_id}\"\n",
        "        if spectrogram_id in parquet_dict:\n",
        "            spectrogram_data = parquet_dict[spectrogram_id]\n",
        "            start_row = int(label_offset_seconds) + 1\n",
        "            selected_data = spectrogram_data[start_row:start_row + 51]\n",
        "\n",
        "            expanded_parquet_dict[unique_key] = selected_data\n",
        "\n",
        "    return expanded_parquet_dict\n",
        "\n",
        "after_expand_parquet_dict = expand_parquet_dict(parquet_dict, df)"
      ],
      "id": "CdWD2ovhAYSu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TsAVswmRBKB"
      },
      "source": [
        "to encode targets"
      ],
      "id": "0TsAVswmRBKB"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sN_FMrf-PeG8"
      },
      "outputs": [],
      "source": [
        "# label encoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "df['expert_consensus_encoded'] = le.fit_transform(df['expert_consensus'])"
      ],
      "id": "sN_FMrf-PeG8"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd2TSaTEPd9-",
        "outputId": "646476d7-81a1-4d05-82bd-3875140b6bbc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['GPD', 'GRDA', 'LPD', 'LRDA', 'Other', 'Seizure'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "le.classes_ # [0,1,2,3,4,5] - target class equivalent by order"
      ],
      "id": "fd2TSaTEPd9-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv0kzWYKRfx9"
      },
      "source": [
        "create df to the split of unique_id and expert_consensus_encoded"
      ],
      "id": "Mv0kzWYKRfx9"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVBlJE12ALBz",
        "outputId": "62648ed2-17a3-4d4c-f6d6-9ca7d803f371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating Matched DataFrame: 100%|██████████| 613/613 [00:00<00:00, 309660.17it/s]\n"
          ]
        }
      ],
      "source": [
        "def create_matched_dataframe(expand_parquet_dict, df):\n",
        "    \"\"\"\n",
        "    Creates a matched DataFrame  to be used in train_test_split by merging an expanded parquet dictionary\n",
        "    with an existing DataFrame based on spectrogram identifiers, and prepares it for model input by encoding\n",
        "    the expert consensus.\n",
        "\n",
        "    This function first extracts keys from the expanded parquet dictionary, which are unique identifiers\n",
        "    consisting of spectrogram IDs and sub-IDs. It then splits these keys to create a new DataFrame with\n",
        "    separate columns for spectrogram IDs and sub-IDs. This new DataFrame is merged with the original DataFrame\n",
        "    (df) on these identifiers to ensure that each row corresponds to the correct spectrogram data.\n",
        "\n",
        "    After merging, the function selects relevant columns for the machine learning model and constructs a\n",
        "    final DataFrame that includes a unique identifier (combining spectrogram ID and sub-ID) and the encoded\n",
        "    expert consensus.\n",
        "\n",
        "    Parameters:\n",
        "    - expand_parquet_dict (dict): A dictionary where keys are unique identifiers (combining spectrogram ID\n",
        "                                  and sub-ID) and values are DataFrames with spectrogram data. This dictionary\n",
        "                                  is created by the `expand_parquet_dict` function.\n",
        "    - df (pd.DataFrame): The original DataFrame that contains 'spectrogram_id', 'spectrogram_sub_id', and\n",
        "                         'expert_consensus_encoded' columns, among others. This DataFrame is used to match\n",
        "                         spectrogram data with its corresponding expert consensus.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A DataFrame ready for splitting into training and testing datasets. It contains two columns:\n",
        "                    'unique_id' (a combination of 'spectrogram_id' and 'spectrogram_sub_id') and fitting\n",
        "                    'expert_consensus_encoded' (the encoded expert consensus labels).\n",
        "\n",
        "    Example usage:\n",
        "    df_to_split = create_matched_dataframe(expand_parquet_dict, df)\n",
        "\n",
        "    \"\"\"\n",
        "    keys = list(expand_parquet_dict.keys())\n",
        "    spectrogram_ids = [key.split('_')[0] for key in tqdm(keys, desc=\"Creating Matched DataFrame\")]\n",
        "    spectrogram_sub_ids = [key.split('_')[1] for key in keys]\n",
        "\n",
        "    matched_df = pd.DataFrame({\n",
        "        'spectrogram_id': spectrogram_ids,\n",
        "        'spectrogram_sub_id': spectrogram_sub_ids\n",
        "    })\n",
        "\n",
        "    matched_df['spectrogram_id'] = matched_df['spectrogram_id'].astype(int)\n",
        "    matched_df['spectrogram_sub_id'] = matched_df['spectrogram_sub_id'].astype(int)\n",
        "\n",
        "    merged_df = pd.merge(matched_df, df, on=['spectrogram_id', 'spectrogram_sub_id'], how='left')\n",
        "\n",
        "    final_df = merged_df[['spectrogram_id', 'spectrogram_sub_id', 'expert_consensus_encoded']]\n",
        "    final_df = final_df.copy()\n",
        "    final_df['unique_id'] = final_df['spectrogram_id'].astype(str) + '_' + final_df['spectrogram_sub_id'].astype(str)\n",
        "    return final_df[['unique_id', 'expert_consensus_encoded']]\n",
        "\n",
        "df_to_split = create_matched_dataframe(after_expand_parquet_dict, df)"
      ],
      "id": "AVBlJE12ALBz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting:"
      ],
      "metadata": {
        "id": "7d2p2HhKqDWk"
      },
      "id": "7d2p2HhKqDWk"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zcMjJiB7Rpsx"
      },
      "outputs": [],
      "source": [
        "X = df_to_split['unique_id']\n",
        "y = df_to_split['expert_consensus_encoded']"
      ],
      "id": "zcMjJiB7Rpsx"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e35hwYGHRpgi",
        "outputId": "1033f6c9-7b66-42e8-b7d4-18a7cfdcaefd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((613,), (613,))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X.shape, y.shape"
      ],
      "id": "e35hwYGHRpgi"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "fROEMBAjRpVU"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "id": "fROEMBAjRpVU"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsnoTwkLRpJW",
        "outputId": "d3acd93b-fc9e-4a4f-fb7c-0eab435355f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((490,), (123,), (490,), (123,))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "id": "JsnoTwkLRpJW"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qjNZvHmHdYye"
      },
      "outputs": [],
      "source": [
        "\n",
        "EXPECTED_NUM_FEATURES = 20000  # coerce amount of features\n",
        "\n",
        "class DataMatcher(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    A custom transformer class for matching and transforming spectrogram data into a consistent format suitable for machine learning models.\n",
        "\n",
        "    This transformer takes a dictionary of expanded parquet data and a list of unique IDs. For each unique ID, it retrieves the corresponding\n",
        "    spectrogram data from the dictionary, flattens it into a 1D array, and ensures that each transformed sample has a consistent number of features.\n",
        "    If the original data has more features than expected, it truncates the array; if it has fewer, it pads the array with zeros. Missing data is\n",
        "    handled by creating an array of zeros.\n",
        "\n",
        "    Parameters:\n",
        "    - expand_parquet_dict (dict): A dictionary where keys are unique identifiers for spectrogram data and values are DataFrames containing the\n",
        "                                  corresponding spectrogram data.\n",
        "\n",
        "    Methods:\n",
        "    - fit(self, X, y=None): Placeholder method for compatibility with sklearn's transformer API. It doesn't learn anything from the data and returns\n",
        "                            the transformer itself.\n",
        "    - transform(self, X): Transforms the provided unique IDs into a numpy array of matched and formatted spectrogram data.\n",
        "\n",
        "    Returns:\n",
        "    - numpy.ndarray: An array where each row corresponds to a flattened and feature-consistent representation of the spectrogram data associated with\n",
        "                     a unique ID in the input list.\n",
        "\n",
        "    Example usage:\n",
        "    data_matcher = DataMatcher(expand_parquet_dict=after_expand_parquet_dict)\n",
        "    X_transformed = data_matcher.transform(X_train_ids)\n",
        "    \"\"\"\n",
        "    def __init__(self, expand_parquet_dict):\n",
        "        self.expand_parquet_dict = expand_parquet_dict\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        matched_data = []\n",
        "        for unique_id in X:\n",
        "            data = self.expand_parquet_dict.get(unique_id)\n",
        "            if data is not None:\n",
        "                flattened_data = data.iloc[1:, 1:].values.flatten()\n",
        "                if len(flattened_data) > EXPECTED_NUM_FEATURES:\n",
        "                    flattened_data = flattened_data[:EXPECTED_NUM_FEATURES]\n",
        "                elif len(flattened_data) < EXPECTED_NUM_FEATURES:\n",
        "                    flattened_data = np.pad(flattened_data, (0, EXPECTED_NUM_FEATURES - len(flattened_data)), 'constant')\n",
        "                matched_data.append(flattened_data)\n",
        "            else:\n",
        "                matched_data.append(np.zeros(EXPECTED_NUM_FEATURES))\n",
        "        matched_data_array = np.vstack(matched_data)\n",
        "        return matched_data_array\n"
      ],
      "id": "qjNZvHmHdYye"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "pipeline and Random Forest Classifier"
      ],
      "metadata": {
        "id": "Kcq6eEYPqTfH"
      },
      "id": "Kcq6eEYPqTfH"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RJRToQZwTIR",
        "outputId": "256ff5ad-b4e0-4a76-e744-edf55aaf7fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 4 4 0 5 4 0 3 3 2 4 0 4 0 4 4 1 4 4 5 5 4 4 4 0 0 0 4 4 5 5 0 4 1 5 0 1\n",
            " 4 4 4 5 1 1 4 4 4 2 1 4 5 5 4 4 4 4 3 4 4 1 0 4 4 0 4 1 4 4 5 4 1 5 4 5 4\n",
            " 4 3 4 4 0 2 4 0 4 0 4 4 4 0 2 4 2 1 1 0 1 0 4 2 4 5 4 5 4 5 0 3 2 4 5 4 2\n",
            " 5 0 2 4 4 4 0 4 4 4 1 3]\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the custom transformer with the expanded parquet dictionary\n",
        "data_matcher = DataMatcher(expand_parquet_dict=after_expand_parquet_dict)\n",
        "\n",
        "# Define the preprocessing and model training steps options: '0' or mean value\n",
        "pipeline_steps = [\n",
        "    ('data_matcher', data_matcher),\n",
        "    # ('imputer', SimpleImputer(strategy='constant', fill_value=0)) # fill NaNs with '0'\n",
        "    ('imputer', SimpleImputer(strategy='mean'))  # fill Nans with mean value\n",
        "\n",
        "]\n",
        "\n",
        "model_pipeline = Pipeline(steps=pipeline_steps + [('classifier', RandomForestClassifier(random_state=42))])\n",
        "\n",
        "X_train_ids = X_train.to_numpy()\n",
        "X_test_ids = X_test.to_numpy()\n",
        "\n",
        "model_pipeline.fit(X_train_ids, y_train)\n",
        "\n",
        "y_pred = model_pipeline.predict(X_test_ids)\n",
        "print(y_pred)"
      ],
      "id": "1RJRToQZwTIR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "metrics:"
      ],
      "metadata": {
        "id": "ftxH210FrWSX"
      },
      "id": "ftxH210FrWSX"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4IMN-GIw4lP",
        "outputId": "e4958e73-ba36-42f8-f87f-65c8114cd621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6585365853658537\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.75      0.82        24\n",
            "           1       0.54      0.64      0.58        11\n",
            "           2       0.78      0.39      0.52        18\n",
            "           3       1.00      0.86      0.92         7\n",
            "           4       0.51      0.81      0.62        36\n",
            "           5       0.78      0.52      0.62        27\n",
            "\n",
            "    accuracy                           0.66       123\n",
            "   macro avg       0.75      0.66      0.68       123\n",
            "weighted avg       0.71      0.66      0.66       123\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "id": "S4IMN-GIw4lP"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inZpwkfczkvc",
        "outputId": "fca119ac-57df-4a64-bde7-1bf67a28be3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    36\n",
              "5    27\n",
              "0    24\n",
              "2    18\n",
              "1    11\n",
              "3     7\n",
              "Name: expert_consensus_encoded, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "y_test.value_counts()"
      ],
      "id": "inZpwkfczkvc"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-or3KvcVw31k",
        "outputId": "83fe1e2a-494f-42d3-d240-b362e76f708f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Loss: 0.8373231601295982\n"
          ]
        }
      ],
      "source": [
        "y_probs = model_pipeline.predict_proba(X_test_ids)\n",
        "\n",
        "logloss = log_loss(y_test, y_probs)\n",
        "print(f\"Log Loss: {logloss}\")"
      ],
      "id": "-or3KvcVw31k"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}