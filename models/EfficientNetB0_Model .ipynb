{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9lGcIDZyMqr",
        "outputId": "4ca24012-d81b-461a-f49a-4515c981e873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Rescaling, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import KLDivergence"
      ],
      "metadata": {
        "id": "25KFMurcGKBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientnet.tfkeras as efn\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import KLDivergence\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build model\n",
        "rn_model = Sequential()\n",
        "\n",
        "# Base EfficientNet model\n",
        "pretrained_model = efn.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights=None,\n",
        "    input_shape=(400, 256, 3)  # Adjust input shape as needed\n",
        ")"
      ],
      "metadata": {
        "id": "L0WV3SADUh1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import efficientnet.tfkeras as efn\n",
        "\n",
        "inp = tf.keras.Input(shape=(400,256,3))\n",
        "base_model = efn.EfficientNetB0(include_top=False, weights=None, input_shape=None)\n",
        "\n",
        "# OUTPUT\n",
        "x = base_model(inp)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(6,activation='softmax', dtype='float32')(x)\n",
        "\n",
        "# COMPILE MODEL\n",
        "model = tf.keras.Model(inputs=inp, outputs=x)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
        "loss = tf.keras.losses.KLDivergence()\n",
        "\n",
        "model.compile(loss=loss, optimizer = opt)"
      ],
      "metadata": {
        "id": "QdYVCJYrXtfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efn_model = Sequential()\n",
        "\n",
        "pretrained_model = tf.keras.applications.efficientnet.EfficientNetB0(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(400, 256, 3),\n",
        "    pooling='average',\n",
        "    classes=6,\n",
        "    classifier_activation='softmax',\n",
        ")\n",
        "\n",
        "for layer in pretrained_model.layers[:-1]:\n",
        "  layer.trainable = False\n",
        "\n",
        "efn_model.add(pretrained_model)\n",
        "efn_model.add(Flatten())\n",
        "efn_model.add(Dense(512, activation='relu'))\n",
        "efn_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "loss = KLDivergence()\n",
        "opt = Adam(learning_rate=0.000000001)\n",
        "callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "efn_model.compile(optimizer=opt, loss=loss)"
      ],
      "metadata": {
        "id": "FhOT0VW0R1ZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "efn_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQc2t1BUcbn1",
        "outputId": "e2a40e72-ab19-4352-9eb4-61a5fb2378cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb0 (Functional  (None, 13, 8, 1280)       4049571   \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 133120)            0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 512)               68157952  \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72210601 (275.46 MB)\n",
            "Trainable params: 68161030 (260.01 MB)\n",
            "Non-trainable params: 4049571 (15.45 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rn_model = Sequential()\n",
        "\n",
        "pretrained_model = keras.applications.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(400, 256, 3),\n",
        "    pooling='average',\n",
        "    classes=6,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "for layer in pretrained_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "rn_model.add(pretrained_model)\n",
        "rn_model.add(Flatten())\n",
        "rn_model.add(Dense(512, activation='relu'))\n",
        "rn_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "rn_model.summary()"
      ],
      "metadata": {
        "id": "RmNjC95ragio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16\n",
        "num_batches = 23  # Calculate total number of batches\n",
        "for i in range(1, num_batches + 1):\n",
        "    batch_path = f'/content/drive/MyDrive/kaggle/X_data_batch_{i*743}.npy'\n",
        "    X_batch = np.load(batch_path)\n",
        "\n",
        "    # Split X_batch into smaller batches of size 16\n",
        "    num_sub_batches = len(X_batch) // batch_size\n",
        "    for j in range(num_sub_batches):\n",
        "        start_idx = j * batch_size\n",
        "        end_idx = (j + 1) * batch_size\n",
        "        X_sub_batch = X_batch[start_idx:end_idx]\n",
        "\n",
        "        # Feed X_sub_batch into your model for training or evaluation\n",
        "        rn_model.fit(X_sub_batch, y_data[(i-1)*len(X_batch) + start_idx:(i-1)*len(X_batch) + end_idx], epochs=1, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "Nwtv1lF5YWsu",
        "outputId": "badf1d12-dde3-438c-bf21-49459302022b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 9s 9s/step - loss: 2.0670 - val_loss: 1.8993\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.4828 - val_loss: nan\n",
            "1/1 [==============================] - 1s 1s/step - loss: nan - val_loss: nan\n",
            "1/1 [==============================] - 1s 1s/step - loss: nan - val_loss: nan\n",
            "1/1 [==============================] - 1s 1s/step - loss: nan - val_loss: nan\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-03320c506431>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Feed X_sub_batch into your model for training or evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mefn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rn_model = Sequential()\n",
        "\n",
        "pretrained_model = keras.applications.ResNet50V2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(400, 256, 3),\n",
        "    pooling='average',\n",
        "    classes=6,\n",
        "    classifier_activation=\"softmax\",\n",
        ")\n",
        "\n",
        "for layer in pretrained_model.layers[:-2]:\n",
        "  layer.trainable = False\n",
        "\n",
        "rn_model.add(pretrained_model)\n",
        "rn_model.add(Flatten())\n",
        "rn_model.add(Dense(512, activation='relu'))\n",
        "rn_model.add(Dense(6, activation='softmax'))\n",
        "\n",
        "rn_model.summary()"
      ],
      "metadata": {
        "id": "K4JIcdIAGRJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90d4f01d-5c40-4028-f130-f0acd24c9474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50v2 (Functional)     (None, 13, 8, 2048)       23564800  \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 212992)            0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 512)               109052416 \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 6)                 3078      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 132620294 (505.91 MB)\n",
            "Trainable params: 109059590 (416.03 MB)\n",
            "Non-trainable params: 23560704 (89.88 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = KLDivergence()\n",
        "opt = Adam(learning_rate=0.000000001)\n",
        "callback = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "rn_model.compile(optimizer=opt, loss=loss)"
      ],
      "metadata": {
        "id": "25T1vxCCJ3gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16\n",
        "num_batches = 23  # Calculate total number of batches\n",
        "for i in range(1, num_batches + 1):\n",
        "    batch_path = f'/content/drive/MyDrive/kaggle/X_data_batch_{i*743}.npy'\n",
        "    X_batch = np.load(batch_path)\n",
        "\n",
        "    # Split X_batch into smaller batches of size 16\n",
        "    num_sub_batches = len(X_batch) // batch_size\n",
        "    for j in range(num_sub_batches):\n",
        "        start_idx = j * batch_size\n",
        "        end_idx = (j + 1) * batch_size\n",
        "        X_sub_batch = X_batch[start_idx:end_idx]\n",
        "\n",
        "        # Feed X_sub_batch into your model for training or evaluation\n",
        "        rn_model.fit(X_sub_batch, y_data[(i-1)*len(X_batch) + start_idx:(i-1)*len(X_batch) + end_idx], epochs=1, validation_split=0.2)  # Example for training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KQNCkzj1NExS",
        "outputId": "efcdd85f-9d2d-409d-fa15-2bc2a27c91c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 17s 17s/step - loss: 2.3111 - val_loss: 3.9989\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3347 - val_loss: 6.9753\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1301 - val_loss: 6.0440\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9024 - val_loss: 6.2959\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7088 - val_loss: 2.5676\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0347 - val_loss: 1.9412\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8241 - val_loss: 3.2498\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7513 - val_loss: 1.9967\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1806 - val_loss: 5.1520\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3877 - val_loss: 2.1502\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5277 - val_loss: 2.4780\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5532 - val_loss: 1.3047\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4663 - val_loss: 1.5445\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3725 - val_loss: 1.5579\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3398 - val_loss: 1.8068\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7943 - val_loss: 2.5046\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7890 - val_loss: 1.2243\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5670 - val_loss: 2.5245\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9256 - val_loss: 1.8267\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3379 - val_loss: 1.5439\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5362 - val_loss: 2.0521\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5105 - val_loss: 1.1865\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.3222 - val_loss: 5.0897\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3714 - val_loss: 1.2835\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.4661 - val_loss: 2.2116\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4774 - val_loss: 3.4292\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2446 - val_loss: 1.5006\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2074 - val_loss: 2.0970\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5002 - val_loss: 1.7576\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3063 - val_loss: 1.4829\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8035 - val_loss: 4.7647\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8284 - val_loss: 1.0847\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6769 - val_loss: 1.2615\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4933 - val_loss: 2.0344\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9729 - val_loss: 1.7186\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6683 - val_loss: 3.1839\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4088 - val_loss: 2.0358\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6552 - val_loss: 1.2093\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1874 - val_loss: 5.0641\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5973 - val_loss: 1.6189\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8592 - val_loss: 0.9519\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6495 - val_loss: 1.1005\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7336 - val_loss: 1.4435\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0656 - val_loss: 1.2728\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5686 - val_loss: 0.7831\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6090 - val_loss: 7.1250\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2544 - val_loss: 1.7558\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3396 - val_loss: 2.0143\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.9044 - val_loss: 1.5774\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4663 - val_loss: 1.7119\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9183 - val_loss: 1.2829\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5952 - val_loss: 1.9027\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2980 - val_loss: 1.1407\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9971 - val_loss: 1.6037\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2250 - val_loss: 1.6416\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5284 - val_loss: 1.6299\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9416 - val_loss: 1.7394\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1353 - val_loss: 1.3526\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3665 - val_loss: 1.6609\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5525 - val_loss: 1.5905\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7383 - val_loss: 1.2306\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6496 - val_loss: 5.0861\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0813 - val_loss: 1.8867\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6997 - val_loss: 1.8834\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7566 - val_loss: 1.0036\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9524 - val_loss: 1.5593\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3335 - val_loss: 1.2991\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.0353 - val_loss: 1.4049\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8034 - val_loss: 1.7327\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1164 - val_loss: 2.0415\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2180 - val_loss: 2.4813\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3944 - val_loss: 1.2632\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4492 - val_loss: 1.5557\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5657 - val_loss: 5.2779\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2357 - val_loss: 1.2224\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.3021 - val_loss: 0.8810\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1357 - val_loss: 1.4441\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3579 - val_loss: 1.1301\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8202 - val_loss: 1.4243\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3244 - val_loss: 1.7540\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2033 - val_loss: 1.5685\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8807 - val_loss: 1.5686\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.1739 - val_loss: 2.2364\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5868 - val_loss: 4.9695\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6044 - val_loss: 1.4074\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7417 - val_loss: 1.5925\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8061 - val_loss: 2.2991\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.2051 - val_loss: 1.2684\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0463 - val_loss: 1.2764\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8427 - val_loss: 1.3528\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8587 - val_loss: 8.7358\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3487 - val_loss: 1.5029\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.5000 - val_loss: 6.6581\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1173 - val_loss: 1.2391\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2095 - val_loss: 1.1034\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3720 - val_loss: 2.0757\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8673 - val_loss: 1.2831\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0782 - val_loss: 2.7947\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0233 - val_loss: 1.2910\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7799 - val_loss: 5.4629\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.1572 - val_loss: 2.0683\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2054 - val_loss: 1.4416\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4743 - val_loss: 1.0761\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2144 - val_loss: 1.8571\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1608 - val_loss: 1.3861\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1338 - val_loss: 1.6516\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.4235 - val_loss: 1.5742\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4286 - val_loss: 1.0613\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1188 - val_loss: 1.3193\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7776 - val_loss: 4.3106\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.2232 - val_loss: 4.9100\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.3127 - val_loss: 0.9425\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3162 - val_loss: 2.8823\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1664 - val_loss: 1.9501\n",
            "1/1 [==============================] - 4s 4s/step - loss: 3.0541 - val_loss: 1.3093\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.0189 - val_loss: 2.1372\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.8449 - val_loss: 1.5704\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6003 - val_loss: 1.4503\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4048 - val_loss: 4.5994\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7850 - val_loss: 1.5043\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5684 - val_loss: 1.2589\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1485 - val_loss: 4.7241\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3718 - val_loss: 1.5095\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2773 - val_loss: 1.4822\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4167 - val_loss: 4.3508\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1451 - val_loss: 5.2510\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3812 - val_loss: 1.4872\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1846 - val_loss: 3.6676\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9394 - val_loss: 3.7174\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2272 - val_loss: 1.5699\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9719 - val_loss: 1.6328\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8222 - val_loss: 1.1617\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6289 - val_loss: 1.3971\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0649 - val_loss: 1.7639\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6444 - val_loss: 1.1528\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6045 - val_loss: 3.3274\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2777 - val_loss: 1.1715\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7311 - val_loss: 1.7530\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9015 - val_loss: 1.5387\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5571 - val_loss: 1.4353\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5710 - val_loss: 1.7859\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7830 - val_loss: 1.6500\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4630 - val_loss: 7.0182\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5271 - val_loss: 2.2517\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5455 - val_loss: 1.8276\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5132 - val_loss: 1.3481\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1411 - val_loss: 1.5044\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.3590 - val_loss: 4.8306\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8476 - val_loss: 1.3913\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3384 - val_loss: 3.8828\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1239 - val_loss: 1.5029\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0238 - val_loss: 4.4365\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3809 - val_loss: 1.8733\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5341 - val_loss: 1.3808\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6758 - val_loss: 1.6500\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3575 - val_loss: 1.4182\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8589 - val_loss: 1.6290\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7133 - val_loss: 1.3968\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8375 - val_loss: 1.8412\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9897 - val_loss: 1.6968\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0383 - val_loss: 1.3879\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8381 - val_loss: 1.4429\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3212 - val_loss: 1.0675\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3322 - val_loss: 1.6888\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0635 - val_loss: 1.5340\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8228 - val_loss: 1.6137\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5488 - val_loss: 1.3149\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.2117 - val_loss: 1.6202\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5531 - val_loss: 1.7385\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9644 - val_loss: 1.0381\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1236 - val_loss: 1.2371\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5167 - val_loss: 1.5500\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7730 - val_loss: 1.3315\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5581 - val_loss: 1.4587\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.3489 - val_loss: 1.8565\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4988 - val_loss: 1.6248\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7668 - val_loss: 1.9501\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4033 - val_loss: 1.5182\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5453 - val_loss: 1.5546\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6614 - val_loss: 1.7293\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9233 - val_loss: 1.2272\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3293 - val_loss: 1.9416\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6945 - val_loss: 1.1542\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.2304 - val_loss: 1.4035\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0729 - val_loss: 1.3739\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6048 - val_loss: 1.4078\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3308 - val_loss: 1.6987\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9271 - val_loss: 1.4149\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5994 - val_loss: 1.3926\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3009 - val_loss: 1.5903\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3023 - val_loss: 1.2898\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5028 - val_loss: 0.9207\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9956 - val_loss: 1.4858\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7043 - val_loss: 1.7191\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8837 - val_loss: 1.4870\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.1098 - val_loss: 1.4295\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9716 - val_loss: 1.6141\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1309 - val_loss: 0.9203\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5336 - val_loss: 1.6602\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3011 - val_loss: 4.1098\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9634 - val_loss: 1.9986\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4116 - val_loss: 5.0421\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7201 - val_loss: 1.2910\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3649 - val_loss: 1.4072\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5514 - val_loss: 1.6423\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8368 - val_loss: 1.5302\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2613 - val_loss: 1.4179\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.4075 - val_loss: 1.0021\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5585 - val_loss: 1.4497\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4586 - val_loss: 1.8666\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9600 - val_loss: 1.2613\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3461 - val_loss: 1.4289\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6168 - val_loss: 1.4554\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2096 - val_loss: 4.7732\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6564 - val_loss: 2.5422\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7361 - val_loss: 1.4499\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0133 - val_loss: 2.0129\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9642 - val_loss: 1.9849\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7791 - val_loss: 1.5486\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9688 - val_loss: 1.6823\n",
            "1/1 [==============================] - 7s 7s/step - loss: 1.8824 - val_loss: 1.6901\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.5360 - val_loss: 3.4096\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.4771 - val_loss: 1.4228\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.7078 - val_loss: 1.8019\n",
            "1/1 [==============================] - 6s 6s/step - loss: 2.4884 - val_loss: 4.8385\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.1397 - val_loss: 1.5417\n",
            "1/1 [==============================] - 4s 4s/step - loss: 2.3718 - val_loss: 1.5551\n",
            "1/1 [==============================] - 5s 5s/step - loss: 2.5068 - val_loss: 1.4565\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.4098 - val_loss: 2.8025\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.4230 - val_loss: 1.4649\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3770 - val_loss: 1.6430\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6026 - val_loss: 1.4295\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0092 - val_loss: 1.4662\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0476 - val_loss: 1.2016\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3132 - val_loss: 3.0161\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0964 - val_loss: 1.5208\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3480 - val_loss: 1.2872\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3389 - val_loss: 1.4792\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0931 - val_loss: 4.0563\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4512 - val_loss: 1.8162\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.2976 - val_loss: 1.7374\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9989 - val_loss: 6.5194\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7148 - val_loss: 1.4051\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5152 - val_loss: 1.3313\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0377 - val_loss: 1.8547\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1695 - val_loss: 1.4062\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8809 - val_loss: 1.6517\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5942 - val_loss: 1.5813\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6231 - val_loss: 1.3455\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1546 - val_loss: 1.5067\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5899 - val_loss: 1.3329\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.4749 - val_loss: 4.0047\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6226 - val_loss: 1.3902\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0043 - val_loss: 1.1258\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.0139 - val_loss: 2.0773\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9194 - val_loss: 1.6766\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1048 - val_loss: 1.1140\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0908 - val_loss: 0.8589\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.6460 - val_loss: 2.4626\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9493 - val_loss: 4.4336\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1439cd61a68d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Feed X_sub_batch into your model for training or evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sub_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Example for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16\n",
        "# Split X_batch into smaller batches of size 16\n",
        "num_sub_batches = len(X_data) // batch_size\n",
        "for j in range(num_sub_batches):\n",
        "    start_idx = j * batch_size\n",
        "    end_idx = (j + 1) * batch_size\n",
        "    X_sub_batch = X_data[start_idx:end_idx]\n",
        "\n",
        "    # Feed X_sub_batch into your model for training\n",
        "    rn_model.fit(X_sub_batch, y_data[start_idx:end_idx], epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLVxV6S6LNNE",
        "outputId": "767b6318-806f-414c-93e4-dd7b12b2953c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 8s 8s/step - loss: 2.3420\n",
            "1/1 [==============================] - 3s 3s/step - loss: 2.5505\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.0968\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8479\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0426\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1487\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9186\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9199\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.9184\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6701\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.5513\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8482\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5267\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7290\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4810\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7375\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.0718\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.5415\n",
            "1/1 [==============================] - 2s 2s/step - loss: 4.0484\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3407\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.7479\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3054\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9871\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7453\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.1673\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1438\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1407\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.9067\n",
            "1/1 [==============================] - 2s 2s/step - loss: 3.2132\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.5043\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.8229\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2829\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5849\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7126\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7824\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5982\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.4948\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6591\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3663\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.6605\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.3003\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.3317\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.7162\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.2412\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0301\n",
            "1/1 [==============================] - 2s 2s/step - loss: 2.8675\n"
          ]
        }
      ]
    }
  ]
}